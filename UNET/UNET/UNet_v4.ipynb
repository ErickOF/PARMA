{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet v4",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-R9o9PR-_-AX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# U-Net Implementation\n",
        "\n",
        "![U-Net Architecture](http://openresearch.ai/uploads/default/optimized/1X/ec0ac2e2d2df8f213b916453375ccee95a254ac3_1_616x500.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G8a-TvyIAHTN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Just for you know, this installation cell won't work on your computer. But, all the following cells will"
      ]
    },
    {
      "metadata": {
        "id": "HfrLO48Q_qPW",
        "colab_type": "code",
        "outputId": "df53b9a6-564f-41c3-a59b-f901860df487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision \n",
        "  \n",
        "!pip install --no-cache-dir -I pillow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 17.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8aiUaQBjARWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup your running folder\n",
        "\n",
        "This cell will mount your folder to use your files like in a normal computer. **MAKE SURE** you change the path with the path of the folder you're working on your Drive."
      ]
    },
    {
      "metadata": {
        "id": "3bPL02jUAO2Z",
        "colab_type": "code",
        "outputId": "4b6bac3f-d9b1-4ff6-fa90-c6660b7dd66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the drive folder. This will prompt for authorization.\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "# Opens the project folder. IMPORTANT: Change to your route\n",
        "%cd 'drive/My Drive/UNet'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/UNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7pb33QipAXnq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modules Importation\n"
      ]
    },
    {
      "metadata": {
        "id": "uu5owO_uAYEP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as Ft\n",
        "\n",
        "from PIL import Image\n",
        "def register_extension(id, extension):\n",
        "    Image.EXTENSION[extension.lower()] = id.upper()\n",
        "Image.register_extension = register_extension\n",
        "def register_extensions(id, extensions): \n",
        "    for extension in extensions:\n",
        "        register_extension(id, extension)\n",
        "Image.register_extensions = register_extensions\n",
        "\n",
        "# Use GPU or not\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iz6SmaiAAfKY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Misc Functions\n",
        "\n",
        "This functions add supportive funcionalities."
      ]
    },
    {
      "metadata": {
        "id": "ykIMluopAf-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_and_print(text):\n",
        "    print(text)\n",
        "    with open(file_Name, 'a') as file:\n",
        "        file.write(text + '\\n')\n",
        "        \n",
        "def time_me(*arg):\n",
        "    if len(arg) != 0: \n",
        "        elapsedTime = time.time() - arg[0];\n",
        "        hours = math.floor(elapsedTime / (60*60))\n",
        "        elapsedTime = elapsedTime - hours * (60*60);\n",
        "        minutes = math.floor(elapsedTime / 60)\n",
        "        elapsedTime = elapsedTime - minutes * (60);\n",
        "        seconds = math.floor(elapsedTime);\n",
        "        elapsedTime = elapsedTime - seconds;\n",
        "        ms = elapsedTime * 1000;\n",
        "        if(hours != 0):\n",
        "            return \"%d hours %d minutes %d seconds\" % (hours, minutes, seconds)\n",
        "        elif(minutes != 0):\n",
        "            return \"%d minutes %d seconds\" % (minutes, seconds)\n",
        "        else :\n",
        "            return \"%d seconds %f ms\" % (seconds, ms)\n",
        "    else:\n",
        "        return time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYzBrx3aAqWm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definition of the Architecture\n",
        "\n",
        "As we show in the above image, U-Net consists on a big amounts of layers. In the following 2 cells all the parts of U-Net will be implemented. Each part have its own comment."
      ]
    },
    {
      "metadata": {
        "id": "PMsJ6cfSAquh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class double_conv(nn.Module):\n",
        "    ''' Applies (conv => BN => ReLU) two times. '''\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            # inplace is for aply ReLU to the original place, saving memory\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            # inplace is for aply ReLU to the original place, saving memory\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    ''' First Section of U-Net. '''\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    ''' Applies a MaxPool with a Kernel of 2x2,\n",
        "        then applies a double convolution pack. '''\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    ''' Applies a Deconvolution and then applies applies a double convolution pack. '''\n",
        "\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super(up, self).__init__()\n",
        "        \n",
        "        # Bilinear is used to save computational cost\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(\n",
        "                scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(\n",
        "                in_ch//2, in_ch//2, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    # the layers on the right are x1 and the ones on the left are x2.\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(input=x2, pad=(diffX // 2, diffX // 2,\n",
        "                                  diffY // 2, diffY // 2))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    ''' Applies the last Convolution to give an answer. '''\n",
        "\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCaXTYcQAwB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    ''' This Object defines the architecture of U-Net. '''\n",
        "\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eLU1ATE6Ayj0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions for Loading the Data\n",
        "\n",
        "The nexts 3 cells are for the loading process. In this case we use the Dataset and Dataloader objects given by Pytorch. In the training process we will call this section"
      ]
    },
    {
      "metadata": {
        "id": "5K3350TXAzEN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BBBCDataset(Dataset):\n",
        "    def __init__(self, ids, dir_data, dir_gt, extension='.png', gt_label='_mask'):\n",
        "\n",
        "        self.dir_data = dir_data\n",
        "        self.dir_gt = dir_gt\n",
        "        self.extension = extension\n",
        "        self.gt_label = gt_label\n",
        "\n",
        "        # Transforms\n",
        "        self.transformations = transforms.ToTensor()\n",
        "\n",
        "        # Images IDS\n",
        "        self.ids = ids\n",
        "\n",
        "        # Calculate len of data\n",
        "        self.data_len = len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get an ID of a specific image\n",
        "        id_img = self.dir_data + self.ids[index] + self.extension\n",
        "        id_gt = self.dir_gt + self.ids[index] + self.extension\n",
        "        # Open Image and GroundTruth\n",
        "        img = Image.open(id_img)\n",
        "        gt = Image.open(id_gt)\n",
        "        # Applies transformations\n",
        "        img = self.transformations(img)\n",
        "        gt = self.transformations(gt)\n",
        "\n",
        "        return (img, gt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yWqZvo-7DCk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_dataloaders(dir_img, dir_gt, test_percent=0.2, batch_size=10):\n",
        "    # Validate a correct percentage\n",
        "    test_percent = test_percent/100 if test_percent > 1 else test_percent\n",
        "    # Read the names of the images\n",
        "    ids = [f[:-4] for f in os.listdir(dir_img)]\n",
        "    # Rearrange the images\n",
        "    random.shuffle(ids)\n",
        "    # Calculate index of partition\n",
        "    part = int(len(ids) * test_percent)\n",
        "    \n",
        "\n",
        "    # Split dataset between train and test\n",
        "    train_ids = ids[part:]\n",
        "    test_ids = ids[:part]\n",
        "\n",
        "    # Create the datasets\n",
        "    train_dataset = BBBCDataset(ids=train_ids, dir_data=dir_img, dir_gt=dir_gt)\n",
        "    test_dataset = BBBCDataset(ids=test_ids, dir_data=dir_img, dir_gt=dir_gt)\n",
        "\n",
        "    # Create the loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2gLhW1D9iJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss functions"
      ]
    },
    {
      "metadata": {
        "id": "W6o6OlEN9rx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight(X):\n",
        "    # K Classes\n",
        "    K = torch.from_numpy(np.unique(X.cpu().detach().numpy())).float().to(device)\n",
        "    W = X.clone()\n",
        "    for k in K:\n",
        "        W[X == k] = W[X == k].size()[0]/X.size()[0]\n",
        "    return W\n",
        "\n",
        "def cross_entropy(y_pred, y_true, W):\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    return torch.mean(W*(-(y_true_f*torch.log10(y_pred_f) + (1 - y_true_f)*torch.log10(1 - y_pred_f)) + 1e-8))\n",
        "\n",
        "def hinge(y_pred, y_true, W):\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    return torch.max(torch.cuda.FloatTensor([0]), torch.mean(W*(1 - y_pred_f*y_true_f)))\n",
        "\n",
        "def huber(y_pred, y_true, W, delta=0.5):\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    delta = torch.cuda.FloatTensor([delta])\n",
        "    _abs = torch.abs(y_true - y_pred)\n",
        "    if _abs.mean() < delta:\n",
        "        return torch.mean(0.5*W*(y_true_f - y_pred_f)**2)\n",
        "    return torch.mean(W*(delta*_abs - 0.5*delta))\n",
        "\n",
        "def MAE(y_pred, y_true, W):\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    return torch.mean(W*torch.abs(y_true_f - y_pred_f))\n",
        "\n",
        "def MSE(y_pred, y_true, W):\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    return torch.mean(W*(y_pred_f - y_true_f)**2)\n",
        "\n",
        "def dice_coef(y_pred, y_true, W):\n",
        "    smooth = 1\n",
        "    y_true_f = y_true.view(-1)\n",
        "    y_pred_f = y_pred.view(-1)\n",
        "    intersection = torch.sum(y_true_f * y_pred_f)\n",
        "    return -W*((2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + smooth))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZDn4ljsADKkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "The next 3 cells are for training the model. Each part is commented."
      ]
    },
    {
      "metadata": {
        "id": "Ytw0uHDbDLCW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_net(net, device, loader, dir_checkpoint, optimizer,\n",
        "              loss_function=dice_coef, epochs=5, run=\"\"):\n",
        "    ''' Train the CNN. '''\n",
        "    loss_results = []\n",
        "    for epoch in range(epochs):\n",
        "        save_and_print('\\nStarting epoch {}/{}.'.format(epoch + 1, epochs))\n",
        "\n",
        "        net.train()\n",
        "        train_loss = 0\n",
        "        cont = 0\n",
        "        time_var = time_me()\n",
        "        for batch_idx, (data, gt) in enumerate(loader):\n",
        "\n",
        "            # Use GPU or not\n",
        "            data, gt = data.to(device, dtype=torch.float), gt.to(device, dtype=torch.float)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            predictions = net(data)\n",
        "\n",
        "            # To calculate Loss\n",
        "            pred_probs = torch.sigmoid(predictions)\n",
        "            pred_probs_flat = pred_probs.view(-1)\n",
        "            gt_flat = gt.view(-1)\n",
        "\n",
        "            # Loss Calculation\n",
        "            W = weight(gt_flat)\n",
        "            loss = loss_function(pred_probs, gt, W).to(device, dtype=torch.float)\n",
        "            train_loss += loss.item()\n",
        "            cont += 1\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            save_and_print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch+1, batch_idx * len(data), len(loader.dataset),\n",
        "                100. * batch_idx / len(loader), loss.item()))\n",
        "\n",
        "        train_loss /= cont\n",
        "        save_and_print('\\nAverage Training Loss: ' + str(train_loss))\n",
        "        save_and_print('Train Time: It tooks '+time_me(time_var)+' to finish the epoch.')\n",
        "        loss_results.append(train_loss)\n",
        "    print(loss_results)\n",
        "    \n",
        "    # Save the weights\n",
        "    #torch.save(net.state_dict(), dir_checkpoint + 'weights'+run+'.pth')\n",
        "        \n",
        "    return train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJ5SjHjtDa8x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_net(net, device, loader, loss_function=dice_coef):\n",
        "    ''' Test the CNN '''\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    cont = 0\n",
        "    time_var = time_me()\n",
        "    with torch.no_grad():\n",
        "        for data, gt in loader:\n",
        "\n",
        "            # Use GPU or not\n",
        "            data, gt = data.to(device, dtype=torch.float), gt.to(device, dtype=torch.float)\n",
        "\n",
        "            # Forward\n",
        "            predictions = net(data)\n",
        "\n",
        "            # To calculate Loss\n",
        "            pred_probs = torch.sigmoid(predictions)\n",
        "            \n",
        "            # Loss Calculation\n",
        "            W = weight(gt.view(-1))\n",
        "            test_loss += loss_function(pred_probs, gt, W).item()\n",
        "            cont += 1\n",
        "\n",
        "    test_loss /= cont\n",
        "    save_and_print('\\nTest set: Average loss: ' + str(test_loss))\n",
        "    save_and_print('Test time: It tooks ' + time_me(time_var) + ' to finish the Test.')\n",
        "    return test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECD2RawrDw2n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def setup_and_run_train(load=False, test_perc=0.2, batch_size=10,\n",
        "                        loss_function=dice_coef, optimizer=optim.Adam, epochs=5,\n",
        "                        lr=0.1, run=\"\"):\n",
        "\n",
        "    # Use GPU or not\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Create the model\n",
        "    net = UNet(n_channels=1, n_classes=1).to(device)\n",
        "\n",
        "    # Load old weights\n",
        "    if load:\n",
        "        net.load_state_dict(torch.load(load))\n",
        "        save_and_print('Model loaded from {}'.format(load))\n",
        "\n",
        "    # Location of the images to use\n",
        "    dir_img = 'data/original/'\n",
        "    dir_gt = 'data/gt/'\n",
        "    dir_checkpoint = 'checkpoints/'\n",
        "\n",
        "    # Load the dataset\n",
        "    train_loader, test_loader = get_dataloaders(dir_img, dir_gt,\n",
        "                                                test_perc, batch_size)\n",
        "    # Optimizer\n",
        "    optimizer = optimizer(net.parameters(), lr=lr)\n",
        "    \n",
        "    # Pretty print of the run\n",
        "    save_and_print('''\n",
        "    Starting training:\n",
        "        Epochs: {}\n",
        "        Batch size: {}\n",
        "        Learning rate: {}\n",
        "        Training size: {}\n",
        "        Testing size: {}\n",
        "        CUDA: {}\n",
        "    '''.format(epochs, batch_size, lr, len(train_loader.dataset),\n",
        "               len(test_loader.dataset), str(use_cuda)))\n",
        "\n",
        "    # Run the training and testing\n",
        "    try:\n",
        "        time_var = time_me()\n",
        "        train_loss = train_net(net=net,\n",
        "                  epochs=epochs,\n",
        "                  device=device,\n",
        "                  dir_checkpoint=dir_checkpoint,\n",
        "                  loader=train_loader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_function=loss_function,\n",
        "                  run = run)\n",
        "        test_loss = test_net(net=net, device=device, loader=test_loader, loss_function=loss_function)\n",
        "        save_and_print('\\nRun time: It tooks ' + time_me(time_var) + ' to finish the run.')\n",
        "        return net\n",
        "    except KeyboardInterrupt:\n",
        "        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n",
        "        save_and_print('Saved interrupt')\n",
        "        try:\n",
        "            sys.exit(0)\n",
        "        except SystemExit:\n",
        "            os._exit(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zFpGI2-BZad3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Result"
      ]
    },
    {
      "metadata": {
        "id": "4-Wp9lcXZh2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_imgs(net, device, loader, optim_name, loss_function_name, show=False):\n",
        "    with torch.no_grad():\n",
        "        i = 0\n",
        "        for batch_idx, (data, gt) in enumerate(loader):\n",
        "            # Use GPU or not\n",
        "            data, gt = data.to(device, dtype=torch.float), gt.to(device, dtype=torch.float)\n",
        "            \n",
        "            if show:\n",
        "                # Shows original image\n",
        "                data_img = transforms.ToPILImage()(data.squeeze(0).cpu()).convert('RGB')\n",
        "                fig=plt.figure(figsize=(20, 20))\n",
        "                fig.add_subplot(2, 2, 1)\n",
        "                plt.imshow(data_img)\n",
        "            \n",
        "            # Forward\n",
        "            predictions = net(data)\n",
        "\n",
        "            # Apply sigmoid\n",
        "            pred_probs = torch.sigmoid(predictions).squeeze(0)\n",
        "            \n",
        "            # Shows prediction\n",
        "            if show:\n",
        "                fig_name = \"outputs/{}_{}{}.png\"\n",
        "                # Shows prediction\n",
        "                pred = transforms.ToPILImage()(predictions.squeeze(0).cpu()).convert('RGB')\n",
        "                fig.add_subplot(2, 2, 2)\n",
        "                plt.savefig(fig_name.format(optim_name, loss_function_name, i))\n",
        "                plt.imshow(pred)\n",
        "                # Shows prediction probability\n",
        "                pred_p = transforms.ToPILImage()(pred_probs.cpu()).convert('RGB')\n",
        "                fig.add_subplot(2, 2, 3)\n",
        "                plt.imshow(pred_p)\n",
        "                plt.savefig(fig_name.format(optim_name, loss_function_name, i))\n",
        "                # Shows gt\n",
        "                gt_img = transforms.ToPILImage()(gt.squeeze(0).cpu()).convert('RGB')\n",
        "                fig.add_subplot(2, 2, 4)\n",
        "                plt.imshow(gt_img)\n",
        "                plt.savefig(fig_name.format(optim_name, loss_function_name, i))\n",
        "                plt.show()\n",
        "            \n",
        "            i += 1;\n",
        "\n",
        "def get_predloader(dir_img, dir_gt, batch_size=1):\n",
        "    # Read the names of the images\n",
        "    ids = [f[:-4] for f in os.listdir(dir_img)]\n",
        "    # Rearrange the images\n",
        "    random.shuffle(ids)\n",
        "    # Calculate index of partition\n",
        "    ids_pred = ids[:10]\n",
        "\n",
        "    # Create the datasets\n",
        "    pred_dataset = BBBCDataset(ids=ids_pred, dir_data=dir_img, dir_gt=dir_gt)\n",
        "\n",
        "    # Create the loaders\n",
        "    pred_loader = DataLoader(pred_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return pred_loader\n",
        "\n",
        "def predict(net, optim_name, loss_function_name):\n",
        "    # Use GPU or not\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Location of the images to use\n",
        "    dir_pred = 'data/original/'\n",
        "    dir_gt = 'data/gt/'\n",
        "\n",
        "    # Load the dataset\n",
        "    pred_loader = get_predloader(dir_pred, dir_gt)\n",
        "\n",
        "    # Run the prediction\n",
        "    predict_imgs(net=net,\n",
        "                 device=device,\n",
        "                 loader=pred_loader,\n",
        "                 optim_name=optim_name,\n",
        "                 loss_function_name=loss_function_name,\n",
        "                 show=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57dr4Mt9EMsZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Running the Training\n",
        "\n",
        "Now, we will run the training. You can change this hyperparameters to see how affects the results.\n",
        "\n",
        "### Only one run"
      ]
    },
    {
      "metadata": {
        "id": "1dmnak7DENPG",
        "colab_type": "code",
        "outputId": "6c61ae79-e149-4b96-c194-1dfc559eb369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7861
        }
      },
      "cell_type": "code",
      "source": [
        "file_Name = \"Prueba.txt\"\n",
        "\n",
        "# Definition of the loss functions\n",
        "loss_functions_names = ['Cross Entropy', 'Hinge', 'Huber', 'MAE', 'MSE',\n",
        "                        'DiceCoefficient']\n",
        "loss_functions = [cross_entropy, hinge, huber, MAE, MSE, dice_coef]\n",
        "\n",
        "# Definition of the optimizers\n",
        "optimizers_names = ['AdaGrad', 'Adadelta', 'Adam', 'Adamax', 'ASGD',\n",
        "                    'RMSprop', 'Rprop', 'SGD']\n",
        "optimizers = [optim.Adagrad, optim.Adadelta, optim.Adam, optim.Adamax,\n",
        "              optim.ASGD, optim.RMSprop, optim.Rprop, optim.SGD]\n",
        "\n",
        "_epochs = 20\n",
        "alpha = 0.0001\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "else:\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "for optimizer, optimizer_name in zip(optimizers, optimizers_names):\n",
        "    save_and_print(optimizer_name)\n",
        "    \n",
        "    for loss_f, loss_f_name in zip(loss_functions, loss_functions_names):\n",
        "        save_and_print(loss_f_name)\n",
        "        net = setup_and_run_train(load = False,\n",
        "                                  test_perc = 0.2,\n",
        "                                  batch_size = 10,\n",
        "                                  loss_function=loss_f,\n",
        "                                  optimizer=optimizer,\n",
        "                                  epochs = _epochs,\n",
        "                                  lr = alpha)\n",
        "        predict(net, optimizer_name, loss_f_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaGrad\n",
            "Cross Entropy\n",
            "\n",
            "    Starting training:\n",
            "        Epochs: 20\n",
            "        Batch size: 10\n",
            "        Learning rate: 0.0001\n",
            "        Training size: 144\n",
            "        Testing size: 36\n",
            "        CUDA: True\n",
            "    \n",
            "\n",
            "Starting epoch 1/20.\n",
            "Train Epoch: 1 [0/144 (0%)]\tLoss: 0.190815\n",
            "Train Epoch: 1 [10/144 (7%)]\tLoss: 0.183809\n",
            "Train Epoch: 1 [20/144 (13%)]\tLoss: 0.178127\n",
            "Train Epoch: 1 [30/144 (20%)]\tLoss: 0.172130\n",
            "Train Epoch: 1 [40/144 (27%)]\tLoss: 0.180138\n",
            "Train Epoch: 1 [50/144 (33%)]\tLoss: 0.179292\n",
            "Train Epoch: 1 [60/144 (40%)]\tLoss: 0.179882\n",
            "Train Epoch: 1 [70/144 (47%)]\tLoss: 0.171134\n",
            "Train Epoch: 1 [80/144 (53%)]\tLoss: 0.176684\n",
            "Train Epoch: 1 [90/144 (60%)]\tLoss: 0.157150\n",
            "Train Epoch: 1 [100/144 (67%)]\tLoss: 0.175878\n",
            "Train Epoch: 1 [110/144 (73%)]\tLoss: 0.167134\n",
            "Train Epoch: 1 [120/144 (80%)]\tLoss: 0.168708\n",
            "Train Epoch: 1 [130/144 (87%)]\tLoss: 0.158068\n",
            "Train Epoch: 1 [56/144 (93%)]\tLoss: 0.152009\n",
            "\n",
            "Average Training Loss: 0.17273051639397938\n",
            "Train Time: It tooks 36 seconds 759.271622 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 2/20.\n",
            "Train Epoch: 2 [0/144 (0%)]\tLoss: 0.154734\n",
            "Train Epoch: 2 [10/144 (7%)]\tLoss: 0.175304\n",
            "Train Epoch: 2 [20/144 (13%)]\tLoss: 0.154815\n",
            "Train Epoch: 2 [30/144 (20%)]\tLoss: 0.154302\n",
            "Train Epoch: 2 [40/144 (27%)]\tLoss: 0.151199\n",
            "Train Epoch: 2 [50/144 (33%)]\tLoss: 0.157840\n",
            "Train Epoch: 2 [60/144 (40%)]\tLoss: 0.151729\n",
            "Train Epoch: 2 [70/144 (47%)]\tLoss: 0.158000\n",
            "Train Epoch: 2 [80/144 (53%)]\tLoss: 0.149742\n",
            "Train Epoch: 2 [90/144 (60%)]\tLoss: 0.145846\n",
            "Train Epoch: 2 [100/144 (67%)]\tLoss: 0.148082\n",
            "Train Epoch: 2 [110/144 (73%)]\tLoss: 0.154075\n",
            "Train Epoch: 2 [120/144 (80%)]\tLoss: 0.149889\n",
            "Train Epoch: 2 [130/144 (87%)]\tLoss: 0.154702\n",
            "Train Epoch: 2 [56/144 (93%)]\tLoss: 0.145948\n",
            "\n",
            "Average Training Loss: 0.15374727149804432\n",
            "Train Time: It tooks 22 seconds 47.223568 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 3/20.\n",
            "Train Epoch: 3 [0/144 (0%)]\tLoss: 0.139955\n",
            "Train Epoch: 3 [10/144 (7%)]\tLoss: 0.156487\n",
            "Train Epoch: 3 [20/144 (13%)]\tLoss: 0.151207\n",
            "Train Epoch: 3 [30/144 (20%)]\tLoss: 0.146215\n",
            "Train Epoch: 3 [40/144 (27%)]\tLoss: 0.147417\n",
            "Train Epoch: 3 [50/144 (33%)]\tLoss: 0.145784\n",
            "Train Epoch: 3 [60/144 (40%)]\tLoss: 0.142842\n",
            "Train Epoch: 3 [70/144 (47%)]\tLoss: 0.145253\n",
            "Train Epoch: 3 [80/144 (53%)]\tLoss: 0.149916\n",
            "Train Epoch: 3 [90/144 (60%)]\tLoss: 0.140470\n",
            "Train Epoch: 3 [100/144 (67%)]\tLoss: 0.147394\n",
            "Train Epoch: 3 [110/144 (73%)]\tLoss: 0.146749\n",
            "Train Epoch: 3 [120/144 (80%)]\tLoss: 0.151769\n",
            "Train Epoch: 3 [130/144 (87%)]\tLoss: 0.143958\n",
            "Train Epoch: 3 [56/144 (93%)]\tLoss: 0.145686\n",
            "\n",
            "Average Training Loss: 0.1467400868733724\n",
            "Train Time: It tooks 22 seconds 56.663990 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 4/20.\n",
            "Train Epoch: 4 [0/144 (0%)]\tLoss: 0.138644\n",
            "Train Epoch: 4 [10/144 (7%)]\tLoss: 0.139252\n",
            "Train Epoch: 4 [20/144 (13%)]\tLoss: 0.150701\n",
            "Train Epoch: 4 [30/144 (20%)]\tLoss: 0.146350\n",
            "Train Epoch: 4 [40/144 (27%)]\tLoss: 0.152483\n",
            "Train Epoch: 4 [50/144 (33%)]\tLoss: 0.148659\n",
            "Train Epoch: 4 [60/144 (40%)]\tLoss: 0.136378\n",
            "Train Epoch: 4 [70/144 (47%)]\tLoss: 0.132265\n",
            "Train Epoch: 4 [80/144 (53%)]\tLoss: 0.137906\n",
            "Train Epoch: 4 [90/144 (60%)]\tLoss: 0.140481\n",
            "Train Epoch: 4 [100/144 (67%)]\tLoss: 0.142900\n",
            "Train Epoch: 4 [110/144 (73%)]\tLoss: 0.152851\n",
            "Train Epoch: 4 [120/144 (80%)]\tLoss: 0.138935\n",
            "Train Epoch: 4 [130/144 (87%)]\tLoss: 0.144329\n",
            "Train Epoch: 4 [56/144 (93%)]\tLoss: 0.135681\n",
            "\n",
            "Average Training Loss: 0.1425210217634837\n",
            "Train Time: It tooks 22 seconds 136.067390 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 5/20.\n",
            "Train Epoch: 5 [0/144 (0%)]\tLoss: 0.158609\n",
            "Train Epoch: 5 [10/144 (7%)]\tLoss: 0.135012\n",
            "Train Epoch: 5 [20/144 (13%)]\tLoss: 0.142496\n",
            "Train Epoch: 5 [30/144 (20%)]\tLoss: 0.144597\n",
            "Train Epoch: 5 [40/144 (27%)]\tLoss: 0.131870\n",
            "Train Epoch: 5 [50/144 (33%)]\tLoss: 0.146148\n",
            "Train Epoch: 5 [60/144 (40%)]\tLoss: 0.148212\n",
            "Train Epoch: 5 [70/144 (47%)]\tLoss: 0.134373\n",
            "Train Epoch: 5 [80/144 (53%)]\tLoss: 0.140093\n",
            "Train Epoch: 5 [90/144 (60%)]\tLoss: 0.146562\n",
            "Train Epoch: 5 [100/144 (67%)]\tLoss: 0.134596\n",
            "Train Epoch: 5 [110/144 (73%)]\tLoss: 0.134797\n",
            "Train Epoch: 5 [120/144 (80%)]\tLoss: 0.128811\n",
            "Train Epoch: 5 [130/144 (87%)]\tLoss: 0.139855\n",
            "Train Epoch: 5 [56/144 (93%)]\tLoss: 0.127547\n",
            "\n",
            "Average Training Loss: 0.13957193990548453\n",
            "Train Time: It tooks 22 seconds 127.024174 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 6/20.\n",
            "Train Epoch: 6 [0/144 (0%)]\tLoss: 0.134309\n",
            "Train Epoch: 6 [10/144 (7%)]\tLoss: 0.127529\n",
            "Train Epoch: 6 [20/144 (13%)]\tLoss: 0.132877\n",
            "Train Epoch: 6 [30/144 (20%)]\tLoss: 0.142505\n",
            "Train Epoch: 6 [40/144 (27%)]\tLoss: 0.140734\n",
            "Train Epoch: 6 [50/144 (33%)]\tLoss: 0.145684\n",
            "Train Epoch: 6 [60/144 (40%)]\tLoss: 0.141800\n",
            "Train Epoch: 6 [70/144 (47%)]\tLoss: 0.138280\n",
            "Train Epoch: 6 [80/144 (53%)]\tLoss: 0.140310\n",
            "Train Epoch: 6 [90/144 (60%)]\tLoss: 0.142295\n",
            "Train Epoch: 6 [100/144 (67%)]\tLoss: 0.131900\n",
            "Train Epoch: 6 [110/144 (73%)]\tLoss: 0.132603\n",
            "Train Epoch: 6 [120/144 (80%)]\tLoss: 0.134276\n",
            "Train Epoch: 6 [130/144 (87%)]\tLoss: 0.140255\n",
            "Train Epoch: 6 [56/144 (93%)]\tLoss: 0.138283\n",
            "\n",
            "Average Training Loss: 0.13757597506046296\n",
            "Train Time: It tooks 22 seconds 119.784594 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 7/20.\n",
            "Train Epoch: 7 [0/144 (0%)]\tLoss: 0.132696\n",
            "Train Epoch: 7 [10/144 (7%)]\tLoss: 0.139089\n",
            "Train Epoch: 7 [20/144 (13%)]\tLoss: 0.144896\n",
            "Train Epoch: 7 [30/144 (20%)]\tLoss: 0.141540\n",
            "Train Epoch: 7 [40/144 (27%)]\tLoss: 0.132065\n",
            "Train Epoch: 7 [50/144 (33%)]\tLoss: 0.148229\n",
            "Train Epoch: 7 [60/144 (40%)]\tLoss: 0.130060\n",
            "Train Epoch: 7 [70/144 (47%)]\tLoss: 0.143810\n",
            "Train Epoch: 7 [80/144 (53%)]\tLoss: 0.135413\n",
            "Train Epoch: 7 [90/144 (60%)]\tLoss: 0.130564\n",
            "Train Epoch: 7 [100/144 (67%)]\tLoss: 0.135205\n",
            "Train Epoch: 7 [110/144 (73%)]\tLoss: 0.134062\n",
            "Train Epoch: 7 [120/144 (80%)]\tLoss: 0.125427\n",
            "Train Epoch: 7 [130/144 (87%)]\tLoss: 0.130066\n",
            "Train Epoch: 7 [56/144 (93%)]\tLoss: 0.133791\n",
            "\n",
            "Average Training Loss: 0.13579411208629608\n",
            "Train Time: It tooks 22 seconds 194.320202 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 8/20.\n",
            "Train Epoch: 8 [0/144 (0%)]\tLoss: 0.133913\n",
            "Train Epoch: 8 [10/144 (7%)]\tLoss: 0.128717\n",
            "Train Epoch: 8 [20/144 (13%)]\tLoss: 0.129304\n",
            "Train Epoch: 8 [30/144 (20%)]\tLoss: 0.144298\n",
            "Train Epoch: 8 [40/144 (27%)]\tLoss: 0.138244\n",
            "Train Epoch: 8 [50/144 (33%)]\tLoss: 0.135871\n",
            "Train Epoch: 8 [60/144 (40%)]\tLoss: 0.135417\n",
            "Train Epoch: 8 [70/144 (47%)]\tLoss: 0.127257\n",
            "Train Epoch: 8 [80/144 (53%)]\tLoss: 0.142914\n",
            "Train Epoch: 8 [90/144 (60%)]\tLoss: 0.132253\n",
            "Train Epoch: 8 [100/144 (67%)]\tLoss: 0.127471\n",
            "Train Epoch: 8 [110/144 (73%)]\tLoss: 0.136441\n",
            "Train Epoch: 8 [120/144 (80%)]\tLoss: 0.128043\n",
            "Train Epoch: 8 [130/144 (87%)]\tLoss: 0.131871\n",
            "Train Epoch: 8 [56/144 (93%)]\tLoss: 0.161174\n",
            "\n",
            "Average Training Loss: 0.13554584781328838\n",
            "Train Time: It tooks 22 seconds 206.803799 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 9/20.\n",
            "Train Epoch: 9 [0/144 (0%)]\tLoss: 0.126806\n",
            "Train Epoch: 9 [10/144 (7%)]\tLoss: 0.120778\n",
            "Train Epoch: 9 [20/144 (13%)]\tLoss: 0.146483\n",
            "Train Epoch: 9 [30/144 (20%)]\tLoss: 0.129314\n",
            "Train Epoch: 9 [40/144 (27%)]\tLoss: 0.133880\n",
            "Train Epoch: 9 [50/144 (33%)]\tLoss: 0.128939\n",
            "Train Epoch: 9 [60/144 (40%)]\tLoss: 0.129841\n",
            "Train Epoch: 9 [70/144 (47%)]\tLoss: 0.140609\n",
            "Train Epoch: 9 [80/144 (53%)]\tLoss: 0.134430\n",
            "Train Epoch: 9 [90/144 (60%)]\tLoss: 0.127216\n",
            "Train Epoch: 9 [100/144 (67%)]\tLoss: 0.136828\n",
            "Train Epoch: 9 [110/144 (73%)]\tLoss: 0.135000\n",
            "Train Epoch: 9 [120/144 (80%)]\tLoss: 0.135149\n",
            "Train Epoch: 9 [130/144 (87%)]\tLoss: 0.138356\n",
            "Train Epoch: 9 [56/144 (93%)]\tLoss: 0.140702\n",
            "\n",
            "Average Training Loss: 0.13362200607856115\n",
            "Train Time: It tooks 22 seconds 228.455067 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 10/20.\n",
            "Train Epoch: 10 [0/144 (0%)]\tLoss: 0.132096\n",
            "Train Epoch: 10 [10/144 (7%)]\tLoss: 0.133670\n",
            "Train Epoch: 10 [20/144 (13%)]\tLoss: 0.130665\n",
            "Train Epoch: 10 [30/144 (20%)]\tLoss: 0.134667\n",
            "Train Epoch: 10 [40/144 (27%)]\tLoss: 0.131073\n",
            "Train Epoch: 10 [50/144 (33%)]\tLoss: 0.128273\n",
            "Train Epoch: 10 [60/144 (40%)]\tLoss: 0.131620\n",
            "Train Epoch: 10 [70/144 (47%)]\tLoss: 0.132033\n",
            "Train Epoch: 10 [80/144 (53%)]\tLoss: 0.127510\n",
            "Train Epoch: 10 [90/144 (60%)]\tLoss: 0.137685\n",
            "Train Epoch: 10 [100/144 (67%)]\tLoss: 0.138091\n",
            "Train Epoch: 10 [110/144 (73%)]\tLoss: 0.136931\n",
            "Train Epoch: 10 [120/144 (80%)]\tLoss: 0.131632\n",
            "Train Epoch: 10 [130/144 (87%)]\tLoss: 0.125186\n",
            "Train Epoch: 10 [56/144 (93%)]\tLoss: 0.127206\n",
            "\n",
            "Average Training Loss: 0.13188923597335817\n",
            "Train Time: It tooks 22 seconds 199.653387 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 11/20.\n",
            "Train Epoch: 11 [0/144 (0%)]\tLoss: 0.129145\n",
            "Train Epoch: 11 [10/144 (7%)]\tLoss: 0.127652\n",
            "Train Epoch: 11 [20/144 (13%)]\tLoss: 0.131061\n",
            "Train Epoch: 11 [30/144 (20%)]\tLoss: 0.127628\n",
            "Train Epoch: 11 [40/144 (27%)]\tLoss: 0.134017\n",
            "Train Epoch: 11 [50/144 (33%)]\tLoss: 0.136707\n",
            "Train Epoch: 11 [60/144 (40%)]\tLoss: 0.134123\n",
            "Train Epoch: 11 [70/144 (47%)]\tLoss: 0.124351\n",
            "Train Epoch: 11 [80/144 (53%)]\tLoss: 0.134153\n",
            "Train Epoch: 11 [90/144 (60%)]\tLoss: 0.131684\n",
            "Train Epoch: 11 [100/144 (67%)]\tLoss: 0.133662\n",
            "Train Epoch: 11 [110/144 (73%)]\tLoss: 0.127123\n",
            "Train Epoch: 11 [120/144 (80%)]\tLoss: 0.138328\n",
            "Train Epoch: 11 [130/144 (87%)]\tLoss: 0.131118\n",
            "Train Epoch: 11 [56/144 (93%)]\tLoss: 0.123643\n",
            "\n",
            "Average Training Loss: 0.13095969458421072\n",
            "Train Time: It tooks 22 seconds 339.516878 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 12/20.\n",
            "Train Epoch: 12 [0/144 (0%)]\tLoss: 0.134327\n",
            "Train Epoch: 12 [10/144 (7%)]\tLoss: 0.129699\n",
            "Train Epoch: 12 [20/144 (13%)]\tLoss: 0.128130\n",
            "Train Epoch: 12 [30/144 (20%)]\tLoss: 0.136039\n",
            "Train Epoch: 12 [40/144 (27%)]\tLoss: 0.128333\n",
            "Train Epoch: 12 [50/144 (33%)]\tLoss: 0.126483\n",
            "Train Epoch: 12 [60/144 (40%)]\tLoss: 0.126869\n",
            "Train Epoch: 12 [70/144 (47%)]\tLoss: 0.136037\n",
            "Train Epoch: 12 [80/144 (53%)]\tLoss: 0.129266\n",
            "Train Epoch: 12 [90/144 (60%)]\tLoss: 0.138995\n",
            "Train Epoch: 12 [100/144 (67%)]\tLoss: 0.136761\n",
            "Train Epoch: 12 [110/144 (73%)]\tLoss: 0.130023\n",
            "Train Epoch: 12 [120/144 (80%)]\tLoss: 0.126971\n",
            "Train Epoch: 12 [130/144 (87%)]\tLoss: 0.125790\n",
            "Train Epoch: 12 [56/144 (93%)]\tLoss: 0.119373\n",
            "\n",
            "Average Training Loss: 0.1302064488331477\n",
            "Train Time: It tooks 22 seconds 153.647661 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 13/20.\n",
            "Train Epoch: 13 [0/144 (0%)]\tLoss: 0.128210\n",
            "Train Epoch: 13 [10/144 (7%)]\tLoss: 0.136997\n",
            "Train Epoch: 13 [20/144 (13%)]\tLoss: 0.135918\n",
            "Train Epoch: 13 [30/144 (20%)]\tLoss: 0.133879\n",
            "Train Epoch: 13 [40/144 (27%)]\tLoss: 0.133723\n",
            "Train Epoch: 13 [50/144 (33%)]\tLoss: 0.124753\n",
            "Train Epoch: 13 [60/144 (40%)]\tLoss: 0.133287\n",
            "Train Epoch: 13 [70/144 (47%)]\tLoss: 0.133799\n",
            "Train Epoch: 13 [80/144 (53%)]\tLoss: 0.129786\n",
            "Train Epoch: 13 [90/144 (60%)]\tLoss: 0.117123\n",
            "Train Epoch: 13 [100/144 (67%)]\tLoss: 0.126949\n",
            "Train Epoch: 13 [110/144 (73%)]\tLoss: 0.135564\n",
            "Train Epoch: 13 [120/144 (80%)]\tLoss: 0.123421\n",
            "Train Epoch: 13 [130/144 (87%)]\tLoss: 0.127874\n",
            "Train Epoch: 13 [56/144 (93%)]\tLoss: 0.132824\n",
            "\n",
            "Average Training Loss: 0.1302738716204961\n",
            "Train Time: It tooks 22 seconds 170.094967 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 14/20.\n",
            "Train Epoch: 14 [0/144 (0%)]\tLoss: 0.131152\n",
            "Train Epoch: 14 [10/144 (7%)]\tLoss: 0.124452\n",
            "Train Epoch: 14 [20/144 (13%)]\tLoss: 0.127089\n",
            "Train Epoch: 14 [30/144 (20%)]\tLoss: 0.128908\n",
            "Train Epoch: 14 [40/144 (27%)]\tLoss: 0.131051\n",
            "Train Epoch: 14 [50/144 (33%)]\tLoss: 0.130989\n",
            "Train Epoch: 14 [60/144 (40%)]\tLoss: 0.134451\n",
            "Train Epoch: 14 [70/144 (47%)]\tLoss: 0.124621\n",
            "Train Epoch: 14 [80/144 (53%)]\tLoss: 0.124407\n",
            "Train Epoch: 14 [90/144 (60%)]\tLoss: 0.126683\n",
            "Train Epoch: 14 [100/144 (67%)]\tLoss: 0.133446\n",
            "Train Epoch: 14 [110/144 (73%)]\tLoss: 0.122928\n",
            "Train Epoch: 14 [120/144 (80%)]\tLoss: 0.132159\n",
            "Train Epoch: 14 [130/144 (87%)]\tLoss: 0.143297\n",
            "Train Epoch: 14 [56/144 (93%)]\tLoss: 0.123227\n",
            "\n",
            "Average Training Loss: 0.12925741026798884\n",
            "Train Time: It tooks 22 seconds 190.437317 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 15/20.\n",
            "Train Epoch: 15 [0/144 (0%)]\tLoss: 0.126952\n",
            "Train Epoch: 15 [10/144 (7%)]\tLoss: 0.129346\n",
            "Train Epoch: 15 [20/144 (13%)]\tLoss: 0.133408\n",
            "Train Epoch: 15 [30/144 (20%)]\tLoss: 0.136203\n",
            "Train Epoch: 15 [40/144 (27%)]\tLoss: 0.124624\n",
            "Train Epoch: 15 [50/144 (33%)]\tLoss: 0.130688\n",
            "Train Epoch: 15 [60/144 (40%)]\tLoss: 0.126502\n",
            "Train Epoch: 15 [70/144 (47%)]\tLoss: 0.128243\n",
            "Train Epoch: 15 [80/144 (53%)]\tLoss: 0.126305\n",
            "Train Epoch: 15 [90/144 (60%)]\tLoss: 0.144522\n",
            "Train Epoch: 15 [100/144 (67%)]\tLoss: 0.127033\n",
            "Train Epoch: 15 [110/144 (73%)]\tLoss: 0.125446\n",
            "Train Epoch: 15 [120/144 (80%)]\tLoss: 0.123440\n",
            "Train Epoch: 15 [130/144 (87%)]\tLoss: 0.125972\n",
            "Train Epoch: 15 [56/144 (93%)]\tLoss: 0.122220\n",
            "\n",
            "Average Training Loss: 0.12872691253821056\n",
            "Train Time: It tooks 22 seconds 238.769531 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 16/20.\n",
            "Train Epoch: 16 [0/144 (0%)]\tLoss: 0.125939\n",
            "Train Epoch: 16 [10/144 (7%)]\tLoss: 0.130452\n",
            "Train Epoch: 16 [20/144 (13%)]\tLoss: 0.124609\n",
            "Train Epoch: 16 [30/144 (20%)]\tLoss: 0.126184\n",
            "Train Epoch: 16 [40/144 (27%)]\tLoss: 0.128769\n",
            "Train Epoch: 16 [50/144 (33%)]\tLoss: 0.122172\n",
            "Train Epoch: 16 [60/144 (40%)]\tLoss: 0.124846\n",
            "Train Epoch: 16 [70/144 (47%)]\tLoss: 0.130716\n",
            "Train Epoch: 16 [80/144 (53%)]\tLoss: 0.127912\n",
            "Train Epoch: 16 [90/144 (60%)]\tLoss: 0.130003\n",
            "Train Epoch: 16 [100/144 (67%)]\tLoss: 0.128827\n",
            "Train Epoch: 16 [110/144 (73%)]\tLoss: 0.130121\n",
            "Train Epoch: 16 [120/144 (80%)]\tLoss: 0.128339\n",
            "Train Epoch: 16 [130/144 (87%)]\tLoss: 0.132211\n",
            "Train Epoch: 16 [56/144 (93%)]\tLoss: 0.142322\n",
            "\n",
            "Average Training Loss: 0.1288948153456052\n",
            "Train Time: It tooks 22 seconds 195.688486 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 17/20.\n",
            "Train Epoch: 17 [0/144 (0%)]\tLoss: 0.130391\n",
            "Train Epoch: 17 [10/144 (7%)]\tLoss: 0.130218\n",
            "Train Epoch: 17 [20/144 (13%)]\tLoss: 0.125958\n",
            "Train Epoch: 17 [30/144 (20%)]\tLoss: 0.127595\n",
            "Train Epoch: 17 [40/144 (27%)]\tLoss: 0.129837\n",
            "Train Epoch: 17 [50/144 (33%)]\tLoss: 0.126226\n",
            "Train Epoch: 17 [60/144 (40%)]\tLoss: 0.126156\n",
            "Train Epoch: 17 [70/144 (47%)]\tLoss: 0.127511\n",
            "Train Epoch: 17 [80/144 (53%)]\tLoss: 0.131952\n",
            "Train Epoch: 17 [90/144 (60%)]\tLoss: 0.126511\n",
            "Train Epoch: 17 [100/144 (67%)]\tLoss: 0.121268\n",
            "Train Epoch: 17 [110/144 (73%)]\tLoss: 0.124288\n",
            "Train Epoch: 17 [120/144 (80%)]\tLoss: 0.129562\n",
            "Train Epoch: 17 [130/144 (87%)]\tLoss: 0.133883\n",
            "Train Epoch: 17 [56/144 (93%)]\tLoss: 0.125496\n",
            "\n",
            "Average Training Loss: 0.12779012968142828\n",
            "Train Time: It tooks 22 seconds 216.943979 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 18/20.\n",
            "Train Epoch: 18 [0/144 (0%)]\tLoss: 0.133114\n",
            "Train Epoch: 18 [10/144 (7%)]\tLoss: 0.121126\n",
            "Train Epoch: 18 [20/144 (13%)]\tLoss: 0.125975\n",
            "Train Epoch: 18 [30/144 (20%)]\tLoss: 0.131936\n",
            "Train Epoch: 18 [40/144 (27%)]\tLoss: 0.133703\n",
            "Train Epoch: 18 [50/144 (33%)]\tLoss: 0.120507\n",
            "Train Epoch: 18 [60/144 (40%)]\tLoss: 0.127974\n",
            "Train Epoch: 18 [70/144 (47%)]\tLoss: 0.128430\n",
            "Train Epoch: 18 [80/144 (53%)]\tLoss: 0.124553\n",
            "Train Epoch: 18 [90/144 (60%)]\tLoss: 0.125956\n",
            "Train Epoch: 18 [100/144 (67%)]\tLoss: 0.126428\n",
            "Train Epoch: 18 [110/144 (73%)]\tLoss: 0.129859\n",
            "Train Epoch: 18 [120/144 (80%)]\tLoss: 0.129699\n",
            "Train Epoch: 18 [130/144 (87%)]\tLoss: 0.128892\n",
            "Train Epoch: 18 [56/144 (93%)]\tLoss: 0.120747\n",
            "\n",
            "Average Training Loss: 0.1272599493463834\n",
            "Train Time: It tooks 22 seconds 300.760269 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 19/20.\n",
            "Train Epoch: 19 [0/144 (0%)]\tLoss: 0.125322\n",
            "Train Epoch: 19 [10/144 (7%)]\tLoss: 0.121013\n",
            "Train Epoch: 19 [20/144 (13%)]\tLoss: 0.130724\n",
            "Train Epoch: 19 [30/144 (20%)]\tLoss: 0.126390\n",
            "Train Epoch: 19 [40/144 (27%)]\tLoss: 0.131856\n",
            "Train Epoch: 19 [50/144 (33%)]\tLoss: 0.128089\n",
            "Train Epoch: 19 [60/144 (40%)]\tLoss: 0.124806\n",
            "Train Epoch: 19 [70/144 (47%)]\tLoss: 0.125917\n",
            "Train Epoch: 19 [80/144 (53%)]\tLoss: 0.125300\n",
            "Train Epoch: 19 [90/144 (60%)]\tLoss: 0.134828\n",
            "Train Epoch: 19 [100/144 (67%)]\tLoss: 0.129974\n",
            "Train Epoch: 19 [110/144 (73%)]\tLoss: 0.124161\n",
            "Train Epoch: 19 [120/144 (80%)]\tLoss: 0.126729\n",
            "Train Epoch: 19 [130/144 (87%)]\tLoss: 0.122128\n",
            "Train Epoch: 19 [56/144 (93%)]\tLoss: 0.131090\n",
            "\n",
            "Average Training Loss: 0.12722177555163702\n",
            "Train Time: It tooks 22 seconds 162.698030 ms to finish the epoch.\n",
            "\n",
            "Starting epoch 20/20.\n",
            "Train Epoch: 20 [0/144 (0%)]\tLoss: 0.133799\n",
            "Train Epoch: 20 [10/144 (7%)]\tLoss: 0.131077\n",
            "Train Epoch: 20 [20/144 (13%)]\tLoss: 0.122429\n",
            "Train Epoch: 20 [30/144 (20%)]\tLoss: 0.128533\n",
            "Train Epoch: 20 [40/144 (27%)]\tLoss: 0.129626\n",
            "Train Epoch: 20 [50/144 (33%)]\tLoss: 0.116724\n",
            "Train Epoch: 20 [60/144 (40%)]\tLoss: 0.124429\n",
            "Train Epoch: 20 [70/144 (47%)]\tLoss: 0.121202\n",
            "Train Epoch: 20 [80/144 (53%)]\tLoss: 0.128211\n",
            "Train Epoch: 20 [90/144 (60%)]\tLoss: 0.128605\n",
            "Train Epoch: 20 [100/144 (67%)]\tLoss: 0.125484\n",
            "Train Epoch: 20 [110/144 (73%)]\tLoss: 0.123746\n",
            "Train Epoch: 20 [120/144 (80%)]\tLoss: 0.125577\n",
            "Train Epoch: 20 [130/144 (87%)]\tLoss: 0.139542\n",
            "Train Epoch: 20 [56/144 (93%)]\tLoss: 0.121854\n",
            "\n",
            "Average Training Loss: 0.1267224927743276\n",
            "Train Time: It tooks 22 seconds 182.800770 ms to finish the epoch.\n",
            "[0.17273051639397938, 0.15374727149804432, 0.1467400868733724, 0.1425210217634837, 0.13957193990548453, 0.13757597506046296, 0.13579411208629608, 0.13554584781328838, 0.13362200607856115, 0.13188923597335817, 0.13095969458421072, 0.1302064488331477, 0.1302738716204961, 0.12925741026798884, 0.12872691253821056, 0.1288948153456052, 0.12779012968142828, 0.1272599493463834, 0.12722177555163702, 0.1267224927743276]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ee8f70fe91d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                   \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                   lr = alpha)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-56ed8c25d713>\u001b[0m in \u001b[0;36msetup_and_run_train\u001b[0;34m(load, test_perc, batch_size, loss_function, optimizer, epochs, lr, run)\u001b[0m\n\u001b[1;32m     49\u001b[0m                   \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   run = run)\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0msave_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nRun time: It tooks '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to finish the run.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f34aaf990790>\u001b[0m in \u001b[0;36mtest_net\u001b[0;34m(net, device, loader, loss_function)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Loss Calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mcont\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5de4aa3467de>\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(y_pred, y_true, W)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_true_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_pred_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_f\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_f\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhinge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (655360) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pgjSI6xjEUWM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Several runs - For result reports"
      ]
    },
    {
      "metadata": {
        "id": "vHv7lrsoEVH9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# file_Name = \"USM-Vainilla.txt\"\n",
        "# runs = 5\n",
        "# acum_train = 0\n",
        "# acum_test = 0\n",
        "# for i in range(runs):\n",
        "#     save_and_print('-'*10 + 'Start run {}'.format(i+1) + '-'*10)\n",
        "#     train_loss, test_loss = setup_and_run_train(load = False,\n",
        "#               test_perc = 0.2,\n",
        "#               batch_size = 10,\n",
        "#               epochs = 20,\n",
        "#               lr = 0.01, run=str(i+1))\n",
        "#     acum_train += train_loss\n",
        "#     acum_test += test_loss\n",
        "\n",
        "# acum_train /= runs\n",
        "# acum_test /= runs\n",
        "\n",
        "# save_and_print('\\nAfter '+str(runs)+' runs: \\n\\tAverage Train Loss: '+str(acum_train)+'\\n\\tAverage Test Loss: '+str(acum_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uXeqBdzAEaTl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seeing the results\n",
        "\n",
        "In the following charts it's the code that uses the trained model and predict some samples."
      ]
    },
    {
      "metadata": {
        "id": "Mg9pGv00EbGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_imgs(net, device, loader, show=False):\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, gt) in enumerate(loader):\n",
        "   \n",
        "            # Use GPU or not\n",
        "            data, gt = data.to(device, dtype=torch.float), gt.to(device, dtype=torch.float)\n",
        "            \n",
        "            \n",
        "            if show:\n",
        "                # Shows original image\n",
        "                data_img = transforms.ToPILImage()(data.squeeze(0).cpu()).convert('RGB')\n",
        "                fig=plt.figure(figsize=(20, 20))\n",
        "                fig.add_subplot(1, 4, 1)\n",
        "                plt.imshow(data_img)\n",
        "            \n",
        "            # Forward\n",
        "            predictions = net(data)\n",
        "\n",
        "            # Apply sigmoid\n",
        "            pred_probs = torch.sigmoid(predictions).squeeze(0)\n",
        "            \n",
        "            # Shows prediction\n",
        "            if show:\n",
        "                # Shows prediction\n",
        "                pred = transforms.ToPILImage()(predictions.squeeze(0).cpu()).convert('RGB')\n",
        "                fig.add_subplot(1, 4, 2)\n",
        "                plt.imshow(pred)\n",
        "                # Shows prediction probability\n",
        "                pred_p = transforms.ToPILImage()(pred_probs.cpu()).convert('RGB')\n",
        "                fig.add_subplot(1, 4, 3)\n",
        "                plt.imshow(pred_p)\n",
        "                # Shows gt\n",
        "                gt_img = transforms.ToPILImage()(gt.squeeze(0).cpu()).convert('RGB')\n",
        "                fig.add_subplot(1, 4, 4)\n",
        "                plt.imshow(gt_img)\n",
        "                plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6sHWE6EbE524",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_predloader(dir_img, dir_gt, batch_size=1):\n",
        "    # Read the names of the images\n",
        "    ids = [f[:-4] for f in os.listdir(dir_img)]\n",
        "    # Rearrange the images\n",
        "    random.shuffle(ids)\n",
        "    # Calculate index of partition\n",
        "    ids_pred = ids[:10]\n",
        "\n",
        "    # Create the datasets\n",
        "    pred_dataset = BBBCDataset(ids=ids_pred, dir_data=dir_img, dir_gt=dir_gt)\n",
        "\n",
        "    # Create the loaders\n",
        "    pred_loader = DataLoader(pred_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return pred_loader\n",
        "\n",
        "def predict(load='checkpoints/CP1.pth'):\n",
        "\n",
        "    # Use GPU or not\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Create the model\n",
        "    net = UNet(n_channels=1, n_classes=1).to(device)\n",
        "\n",
        "    # Load trained weights\n",
        "    net.load_state_dict(torch.load(load))\n",
        "    print('Model loaded from {}'.format(load))\n",
        "\n",
        "    # Location of the images to use\n",
        "    dir_pred = 'data/original/'\n",
        "    dir_gt = 'data/gt/'\n",
        "\n",
        "    # Load the dataset\n",
        "    pred_loader = get_predloader(dir_pred, dir_gt)\n",
        "\n",
        "    # Run the prediction\n",
        "    predict_imgs(net=net,\n",
        "                device=device,\n",
        "                loader=pred_loader,\n",
        "                show=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "032NVEA0E8Df",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict(load='checkpoints/weights.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}